{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K.image_dim_ordering: tf\n",
      "Loaded vgg16 model\n"
     ]
    }
   ],
   "source": [
    "'''Code for fine-tuning Inception V3 for a new task.\n",
    "\n",
    "Start with Inception V3 network, not including last fully connected layers.\n",
    "\n",
    "Train a simple fully connected layer on top of these.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import random\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "import inception_v3 as inception\n",
    "import vgg16 as VGG\n",
    "import prepare.collect as pc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "set_session(tf.Session(config=config))\n",
    "'''\n",
    "N_CLASSES = 2\n",
    "IMSIZE = (224, 224)\n",
    "\n",
    "XML_DIR = \"../data/annotations/xmls/\"\n",
    "IMG_DIR = \"../data/images/\"\n",
    "VAL_RATIO = 0.3\n",
    "\n",
    "# TO DO:: Replace these with paths to the downloaded data.\n",
    "# Training directory\n",
    "# train_dir = '../data/catdog/train'\n",
    "# Testing directory\n",
    "# test_dir = '../data/catdog/validation'\n",
    "\n",
    "\n",
    "# Start with an Inception V3 model, not including the final softmax layer.\n",
    "base_model = VGG.VGG16(weights='imagenet')\n",
    "print ('Loaded vgg16 model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turn off training on base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "\n",
    "# Add on new fully connected layers for the output classes.\n",
    "# x = Dense(1024, activation='relu')(base_model.get_layer('fc2').output)\n",
    "# x = Dropout(0.5)(x)\n",
    "# predictions = Dense(N_CLASSES, activation='softmax', name='predictions')(x)\n",
    "\n",
    "base_model_last = base_model.get_layer('flatten').output\n",
    "x = Dense(4096, activation='relu', name='fc1-1')(base_model_last)\n",
    "x = Dense(4096, activation='relu', name='fc1-2')(x)\n",
    "predictions = Dense(N_CLASSES, activation='softmax', name='predictions')(x)\n",
    "\n",
    "# model = Model(input=base_model.input, output=predictions)\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "y = Dense(4096, activation='relu', name='fc2-1')(base_model_last)\n",
    "y = Dense(4096, activation='linear', name='fc2-2')(y)\n",
    "aux_predictions = Dense(4, activation='linear', name='aux_predictions')(y)\n",
    "\n",
    "#model = Model(input=base_model.input, output=predictions)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model = Model(input=base_model.input, output=[predictions, aux_predictions])\n",
    "\n",
    "#not training bbox part\n",
    "model.get_layer(\"fc2-1\").trainable=False\n",
    "model.get_layer(\"fc2-2\").trainable=False\n",
    "model.get_layer(\"aux_predictions\").trainable=False\n",
    "\n",
    "#adam = optimizers.Adam(lr=0.00001)\n",
    "sgd = optimizers.SGD(lr=0.00001)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss={'predictions': 'categorical_crossentropy', 'aux_predictions': 'mean_squared_error'},\n",
    "             loss_weights={'predictions': 1, 'aux_predictions': 0}, metrics=['accuracy'])\n",
    "\n",
    "# model = Model(input=base_model.input, output=aux_predictions)\n",
    "# model.load_weights('catdog_combine.h5',by_name=True) \n",
    "\n",
    "\n",
    "#model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Convolution2D)     (None, 224, 224, 64)  1792        input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Convolution2D)     (None, 224, 224, 64)  36928       block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, 112, 112, 64)  0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Convolution2D)     (None, 112, 112, 128) 73856       block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Convolution2D)     (None, 112, 112, 128) 147584      block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, 56, 56, 128)   0           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Convolution2D)     (None, 56, 56, 256)   295168      block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Convolution2D)     (None, 56, 56, 256)   590080      block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Convolution2D)     (None, 56, 56, 256)   590080      block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, 28, 28, 256)   0           block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Convolution2D)     (None, 28, 28, 512)   1180160     block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Convolution2D)     (None, 28, 28, 512)   2359808     block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3 (Convolution2D)     (None, 28, 28, 512)   2359808     block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, 14, 14, 512)   0           block4_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Convolution2D)     (None, 14, 14, 512)   2359808     block4_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Convolution2D)     (None, 14, 14, 512)   2359808     block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3 (Convolution2D)     (None, 14, 14, 512)   2359808     block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)       (None, 7, 7, 512)     0           block5_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten (Flatten)                (None, 25088)         0           block5_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "fc1-1 (Dense)                    (None, 4096)          102764544   flatten[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "fc2-1 (Dense)                    (None, 4096)          102764544   flatten[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "fc1-2 (Dense)                    (None, 4096)          16781312    fc1-1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "fc2-2 (Dense)                    (None, 4096)          16781312    fc2-1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "predictions (Dense)              (None, 2)             8194        fc1-2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "aux_predictions (Dense)          (None, 4)             16388       fc2-2[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 253,830,982\n",
      "Trainable params: 119,554,050\n",
      "Non-trainable params: 134,276,932\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Show some debug output\n",
    "print (model.summary())\n",
    "\n",
    "# print ('Trainable weights')\n",
    "#model.save_weights('catdog_pretrain.h5')\n",
    "\n",
    "#print (model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for oxford dataset\n",
    "xmlFiles = pc.listAllFiles(XML_DIR)\n",
    "infoList = list(map(lambda f:pc.getInfoTupleForXml(f,IMG_DIR) ,xmlFiles))\n",
    "\n",
    "random.shuffle(infoList)\n",
    "cutIndex = int(len(infoList)*VAL_RATIO)\n",
    "train_files_ox = infoList[:cutIndex]\n",
    "val_files_ox = infoList[cutIndex:]\n",
    "#------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for kaggle dataset\n",
    "KAGGLE_TRAIN_DIR=\"../../Mind/A3/data/catdog/train/\"\n",
    "KAGGLE_VAL_DIR=\"../../Mind/A3/data/catdog/validation/\"\n",
    "def processKaggleFolder(imgDir, label):\n",
    "    files = pc.listAllFiles(imgDir)\n",
    "    files = list(map(lambda f:[0,f,label,(50,50,50,50)],files))\n",
    "    return files\n",
    "catfiles_train=processKaggleFolder (KAGGLE_TRAIN_DIR+\"cat/\",0)\n",
    "dogfiles_train =processKaggleFolder (KAGGLE_TRAIN_DIR+\"dog/\",1)\n",
    "train_files_kaggle = catfiles_train + dogfiles_train\n",
    "catfiles_val = processKaggleFolder (KAGGLE_VAL_DIR+\"cat/\",0)\n",
    "dogfiles_val = processKaggleFolder (KAGGLE_VAL_DIR+\"dog/\",1)\n",
    "val_files_kaggle = catfiles_val+dogfiles_val\n",
    "train_files = train_files_ox + train_files_kaggle\n",
    "val_files = val_files_ox+ val_files_kaggle\n",
    "np.random.seed()\n",
    "random.shuffle(train_files)\n",
    "random.shuffle(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#print(val_files)\n",
    "#np.random.seed()\n",
    "img_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "def to_categorical(y, num_classes=None):\n",
    "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
    "    E.g. for use with categorical_crossentropy.\n",
    "    # Arguments\n",
    "        y: class vector to be converted into a matrix\n",
    "            (integers from 0 to num_classes).\n",
    "        num_classes: total number of classes.\n",
    "    # Returns\n",
    "        A binary matrix representation of the input.\n",
    "    \"\"\"\n",
    "    y = np.array(y, dtype='int').ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes))\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    return categorical\n",
    "\n",
    "def my_load_img(img_path,img_datagen,size):\n",
    "    img = image.load_img(img_path, target_size=size)\n",
    "    x = image.img_to_array(img)\n",
    "\n",
    "#     x = img_datagen.img_to_array(img)\n",
    "    x = img_datagen.random_transform(x)\n",
    "    x = img_datagen.standardize(x)\n",
    "    #x = np.expand_dims(x, axis=0)\n",
    "    return x\n",
    "def my_img_generator(files,img_datagen,batch_size):\n",
    "#     index_array = np.random.permutation(len(files))\n",
    "    \n",
    "    index = 0\n",
    "    count = 0\n",
    "    img_datas=[]\n",
    "    img_labels=[]\n",
    "    img_bboxes=[]\n",
    "    while 1:\n",
    "        # create numpy arrays of input data\n",
    "        # and labels, from each line in the file\n",
    "        if count < batch_size:\n",
    "            img_datas.append(my_load_img(files[index][1],img_datagen,IMSIZE))\n",
    "    #                 lable=[0.0,0.0]\n",
    "    #                 lable[files[index][1]]=1.0\n",
    "            img_labels.append(files[index][2])\n",
    "            \n",
    "            img_bboxes.append(np.array(files[index][3]))\n",
    "            \n",
    "            index=(index+1)%len(files)\n",
    "            count+=1\n",
    "        else:\n",
    "            count=0\n",
    "            #print(img_datas)\n",
    "            one_hot_labels=to_categorical(img_labels, num_classes=2)\n",
    "            #print(img_bboxes)\n",
    "            yield (np.array(img_datas),[np.array(one_hot_labels),np.array(img_bboxes)])\n",
    "                # yield (np.array(img_datas),np.array(img_bboxes))\n",
    "#             else:\n",
    "#                 yield (np.array(img_datas),np.array(one_hot_labels))\n",
    "            img_datas = []\n",
    "            img_labels = []\n",
    "            img_bboxes=[]\n",
    "#             random.shuffle(files)\n",
    "            \n",
    "\n",
    "batch_size=32\n",
    "# t = next(my_img_generator(train_files,img_datagen,batch_size))\n",
    "\n",
    "# model.load_weights('catdog_pretrain_nf.h5') \n",
    "# train_data\n",
    "# train_data.shape\n",
    "my_train_generator = my_img_generator(train_files,img_datagen,batch_size)\n",
    "my_val_generator = my_img_generator(val_files,img_datagen,batch_size)\n",
    "\n",
    "#train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#         train_dir,  # this is the target directory\n",
    "#         target_size=IMSIZE,  # all images will be resized to 299x299 Inception V3 input\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='categorical')\n",
    "\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# test_generator = test_datagen.flow_from_directory(\n",
    "#         test_dir,  # this is the target directory\n",
    "#         target_size=IMSIZE,  # all images will be resized to 299x299 Inception V3 input\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='categorical')\n",
    "\n",
    "#print(next(my_train_generator)[1])\n",
    "# print(a[1].shape)\n",
    "# print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('catdog_combine_2.h5',by_name=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6s - loss: 0.1359 - predictions_loss: 0.1359 - aux_predictions_loss: 672.0142 - predictions_acc: 0.9531 - aux_predictions_acc: 0.0547 - val_loss: 0.1385 - val_predictions_loss: 0.1385 - val_aux_predictions_loss: 510.3819 - val_predictions_acc: 0.9375 - val_aux_predictions_acc: 0.2734\n",
      "Epoch 2/100\n",
      "3s - loss: 0.0984 - predictions_loss: 0.0984 - aux_predictions_loss: 690.6973 - predictions_acc: 0.9531 - aux_predictions_acc: 0.0391 - val_loss: 0.1154 - val_predictions_loss: 0.1154 - val_aux_predictions_loss: 496.2886 - val_predictions_acc: 0.9688 - val_aux_predictions_acc: 0.2812\n",
      "Epoch 3/100\n",
      "3s - loss: 0.1222 - predictions_loss: 0.1222 - aux_predictions_loss: 658.7384 - predictions_acc: 0.9688 - aux_predictions_acc: 0.0469 - val_loss: 0.2421 - val_predictions_loss: 0.2421 - val_aux_predictions_loss: 475.2512 - val_predictions_acc: 0.8750 - val_aux_predictions_acc: 0.2656\n",
      "Epoch 4/100\n",
      "3s - loss: 0.0854 - predictions_loss: 0.0854 - aux_predictions_loss: 666.7386 - predictions_acc: 0.9766 - aux_predictions_acc: 0.0625 - val_loss: 0.2250 - val_predictions_loss: 0.2250 - val_aux_predictions_loss: 516.9474 - val_predictions_acc: 0.9141 - val_aux_predictions_acc: 0.2266\n",
      "Epoch 5/100\n",
      "3s - loss: 0.1349 - predictions_loss: 0.1349 - aux_predictions_loss: 631.0628 - predictions_acc: 0.9453 - aux_predictions_acc: 0.0859 - val_loss: 0.1902 - val_predictions_loss: 0.1902 - val_aux_predictions_loss: 485.2754 - val_predictions_acc: 0.9219 - val_aux_predictions_acc: 0.2266\n",
      "Epoch 6/100\n",
      "3s - loss: 0.1780 - predictions_loss: 0.1780 - aux_predictions_loss: 650.3381 - predictions_acc: 0.9219 - aux_predictions_acc: 0.0391 - val_loss: 0.2552 - val_predictions_loss: 0.2552 - val_aux_predictions_loss: 492.0276 - val_predictions_acc: 0.8828 - val_aux_predictions_acc: 0.2812\n",
      "Epoch 7/100\n",
      "3s - loss: 0.1350 - predictions_loss: 0.1350 - aux_predictions_loss: 677.9526 - predictions_acc: 0.9375 - aux_predictions_acc: 0.0469 - val_loss: 0.1348 - val_predictions_loss: 0.1348 - val_aux_predictions_loss: 531.1915 - val_predictions_acc: 0.9531 - val_aux_predictions_acc: 0.2812\n",
      "Epoch 8/100\n",
      "3s - loss: 0.0725 - predictions_loss: 0.0725 - aux_predictions_loss: 705.3895 - predictions_acc: 0.9844 - aux_predictions_acc: 0.0469 - val_loss: 0.1581 - val_predictions_loss: 0.1581 - val_aux_predictions_loss: 479.5633 - val_predictions_acc: 0.9375 - val_aux_predictions_acc: 0.3203\n",
      "Epoch 9/100\n",
      "3s - loss: 0.1570 - predictions_loss: 0.1570 - aux_predictions_loss: 618.8448 - predictions_acc: 0.9531 - aux_predictions_acc: 0.0391 - val_loss: 0.1888 - val_predictions_loss: 0.1888 - val_aux_predictions_loss: 478.6002 - val_predictions_acc: 0.9219 - val_aux_predictions_acc: 0.3125\n",
      "Epoch 10/100\n",
      "3s - loss: 0.1366 - predictions_loss: 0.1366 - aux_predictions_loss: 666.0685 - predictions_acc: 0.9453 - aux_predictions_acc: 0.0547 - val_loss: 0.2611 - val_predictions_loss: 0.2611 - val_aux_predictions_loss: 430.0869 - val_predictions_acc: 0.9062 - val_aux_predictions_acc: 0.3672\n",
      "Epoch 11/100\n",
      "3s - loss: 0.1350 - predictions_loss: 0.1350 - aux_predictions_loss: 663.6482 - predictions_acc: 0.9531 - aux_predictions_acc: 0.0312 - val_loss: 0.1556 - val_predictions_loss: 0.1556 - val_aux_predictions_loss: 474.4774 - val_predictions_acc: 0.9375 - val_aux_predictions_acc: 0.2578\n",
      "Epoch 12/100\n",
      "3s - loss: 0.0665 - predictions_loss: 0.0665 - aux_predictions_loss: 691.8515 - predictions_acc: 0.9688 - aux_predictions_acc: 0.0234 - val_loss: 0.1739 - val_predictions_loss: 0.1739 - val_aux_predictions_loss: 523.6013 - val_predictions_acc: 0.9297 - val_aux_predictions_acc: 0.2500\n",
      "Epoch 13/100\n",
      "3s - loss: 0.0811 - predictions_loss: 0.0811 - aux_predictions_loss: 669.0376 - predictions_acc: 0.9766 - aux_predictions_acc: 0.0391 - val_loss: 0.2354 - val_predictions_loss: 0.2354 - val_aux_predictions_loss: 435.6563 - val_predictions_acc: 0.9062 - val_aux_predictions_acc: 0.2969\n",
      "Epoch 14/100\n",
      "3s - loss: 0.1555 - predictions_loss: 0.1555 - aux_predictions_loss: 645.2000 - predictions_acc: 0.9297 - aux_predictions_acc: 0.0312 - val_loss: 0.2203 - val_predictions_loss: 0.2203 - val_aux_predictions_loss: 466.1168 - val_predictions_acc: 0.9297 - val_aux_predictions_acc: 0.2891\n",
      "Epoch 15/100\n",
      "3s - loss: 0.1102 - predictions_loss: 0.1102 - aux_predictions_loss: 649.7469 - predictions_acc: 0.9922 - aux_predictions_acc: 0.0469 - val_loss: 0.1369 - val_predictions_loss: 0.1369 - val_aux_predictions_loss: 514.4186 - val_predictions_acc: 0.9297 - val_aux_predictions_acc: 0.2578\n",
      "Epoch 16/100\n",
      "3s - loss: 0.1270 - predictions_loss: 0.1270 - aux_predictions_loss: 667.8035 - predictions_acc: 0.9453 - aux_predictions_acc: 0.0312 - val_loss: 0.1722 - val_predictions_loss: 0.1722 - val_aux_predictions_loss: 496.2734 - val_predictions_acc: 0.9297 - val_aux_predictions_acc: 0.2734\n",
      "Epoch 17/100\n",
      "3s - loss: 0.2062 - predictions_loss: 0.2062 - aux_predictions_loss: 677.0929 - predictions_acc: 0.9297 - aux_predictions_acc: 0.0469 - val_loss: 0.1286 - val_predictions_loss: 0.1286 - val_aux_predictions_loss: 441.7069 - val_predictions_acc: 0.9453 - val_aux_predictions_acc: 0.2812\n",
      "Epoch 18/100\n",
      "3s - loss: 0.1150 - predictions_loss: 0.1150 - aux_predictions_loss: 631.8701 - predictions_acc: 0.9609 - aux_predictions_acc: 0.0625 - val_loss: 0.1163 - val_predictions_loss: 0.1163 - val_aux_predictions_loss: 543.3624 - val_predictions_acc: 0.9531 - val_aux_predictions_acc: 0.2578\n",
      "Epoch 19/100\n",
      "3s - loss: 0.1323 - predictions_loss: 0.1323 - aux_predictions_loss: 657.6384 - predictions_acc: 0.9375 - aux_predictions_acc: 0.0234 - val_loss: 0.2133 - val_predictions_loss: 0.2133 - val_aux_predictions_loss: 489.3773 - val_predictions_acc: 0.9219 - val_aux_predictions_acc: 0.2422\n",
      "Epoch 20/100\n",
      "3s - loss: 0.0960 - predictions_loss: 0.0960 - aux_predictions_loss: 610.7856 - predictions_acc: 0.9688 - aux_predictions_acc: 0.0391 - val_loss: 0.1890 - val_predictions_loss: 0.1890 - val_aux_predictions_loss: 470.2733 - val_predictions_acc: 0.9297 - val_aux_predictions_acc: 0.3125\n",
      "Epoch 21/100\n",
      "3s - loss: 0.0724 - predictions_loss: 0.0724 - aux_predictions_loss: 657.2228 - predictions_acc: 1.0000 - aux_predictions_acc: 0.0469 - val_loss: 0.2847 - val_predictions_loss: 0.2847 - val_aux_predictions_loss: 456.3122 - val_predictions_acc: 0.8594 - val_aux_predictions_acc: 0.3047\n",
      "Epoch 22/100\n",
      "3s - loss: 0.1344 - predictions_loss: 0.1344 - aux_predictions_loss: 682.9776 - predictions_acc: 0.9531 - aux_predictions_acc: 0.0547 - val_loss: 0.2878 - val_predictions_loss: 0.2878 - val_aux_predictions_loss: 475.4779 - val_predictions_acc: 0.9141 - val_aux_predictions_acc: 0.2969\n",
      "Epoch 23/100\n",
      "3s - loss: 0.1351 - predictions_loss: 0.1351 - aux_predictions_loss: 657.5645 - predictions_acc: 0.9453 - aux_predictions_acc: 0.0469 - val_loss: 0.1282 - val_predictions_loss: 0.1282 - val_aux_predictions_loss: 473.9831 - val_predictions_acc: 0.9453 - val_aux_predictions_acc: 0.3047\n",
      "Epoch 24/100\n",
      "3s - loss: 0.1188 - predictions_loss: 0.1188 - aux_predictions_loss: 690.8748 - predictions_acc: 0.9609 - aux_predictions_acc: 0.0234 - val_loss: 0.1481 - val_predictions_loss: 0.1481 - val_aux_predictions_loss: 417.9581 - val_predictions_acc: 0.9297 - val_aux_predictions_acc: 0.3438\n",
      "Epoch 25/100\n",
      "3s - loss: 0.0992 - predictions_loss: 0.0992 - aux_predictions_loss: 646.5672 - predictions_acc: 0.9688 - aux_predictions_acc: 0.0469 - val_loss: 0.1582 - val_predictions_loss: 0.1582 - val_aux_predictions_loss: 520.1830 - val_predictions_acc: 0.9297 - val_aux_predictions_acc: 0.2734\n",
      "Epoch 26/100\n",
      "3s - loss: 0.1014 - predictions_loss: 0.1014 - aux_predictions_loss: 668.5767 - predictions_acc: 0.9766 - aux_predictions_acc: 0.0234 - val_loss: 0.1845 - val_predictions_loss: 0.1845 - val_aux_predictions_loss: 471.0037 - val_predictions_acc: 0.9297 - val_aux_predictions_acc: 0.2500\n",
      "Epoch 27/100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# my_train_generator = my_img_generator(train_files,img_datagen,32)\n",
    "# my_val_generator = my_img_generator(val_files,img_datagen,32)\n",
    "# model.fit_generator(\n",
    "#         my_train_generator,\n",
    "#         samples_per_epoch=128,\n",
    "#         nb_epoch=10,\n",
    "#         validation_data=test_datagen,\n",
    "#         verbose=2,\n",
    "#         nb_val_samples=128)\n",
    "\n",
    "model.fit_generator(\n",
    "        my_train_generator,\n",
    "        samples_per_epoch=128,\n",
    "        nb_epoch=100,\n",
    "        validation_data=my_val_generator,\n",
    "        verbose=2,\n",
    "        nb_val_samples=128)\n",
    "model.save_weights('catdog_combine_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# img_path = '../data/cat2.jpg'\n",
    "test_img = train_files[0]\n",
    "img_path = test_img[1]\n",
    "# img = image.load_img(img_path, target_size=IMSIZE)\n",
    "# x = image.img_to_array(img)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# x = inception.preprocess_input(x)\n",
    "# x = my_load_img(img_path,img_datagen,IMSIZE)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "# preds = model.predict(x)[0]\n",
    "# print('Predicted:', preds)\n",
    "# width=test_img[4][0]\n",
    "# height= test_img[4][1]\n",
    "# actual_preds=[preds[0]*width*0.01,preds[1]*height*0.01,preds[2]*width*0.01,preds[3]*height*0.01]\n",
    "# print('Actual_preds',actual_preds)\n",
    "\n",
    "def getPredForImg(img_path):\n",
    "    x = my_load_img(img_path,img_datagen,IMSIZE)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    preds = model.predict(x)\n",
    "    return preds\n",
    "def testImage(img_path,preds):  \n",
    "    size = Image.open(img_path).size\n",
    "    width=size[0]\n",
    "    height=size[1]\n",
    "    im = np.array(Image.open(img_path), dtype=np.uint8)\n",
    "    # Create figure and axes\n",
    "    fig,ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "    actual_preds=[preds[0]*width*0.01,preds[1]*height*0.01,preds[2]*width*0.01,preds[3]*height*0.01]\n",
    "    # Create a Rectangle patch\n",
    "    rect = patches.Rectangle((actual_preds[0],actual_preds[1]),actual_preds[2]-actual_preds[0],actual_preds[3]-actual_preds[1],linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "    plt.show()\n",
    "    return\n",
    "files = pc.listAllFiles(\"../data/test/\")\n",
    "random.shuffle(files)\n",
    "for f in files[60:70]:\n",
    "    preds = getPredForImg(f)\n",
    "    print(preds)\n",
    "    testImage(f,preds[1][0])\n",
    "    \n",
    "preds = getPredForImg(\"../data/human2.jpg\")\n",
    "testImage(\"../data/human2.jpg\",preds[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image.open(img_path).size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
